{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>turn1</th>\n",
       "      <th>turn2</th>\n",
       "      <th>turn3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "      <td>dont worry ism girl</td>\n",
       "      <td>hmm how do i know if you are</td>\n",
       "      <td>whats ur name ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>angry</td>\n",
       "      <td>when did in</td>\n",
       "      <td>saw many times i think</td>\n",
       "      <td>no . i never saw you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>others</td>\n",
       "      <td>by</td>\n",
       "      <td>by google chrome</td>\n",
       "      <td>where you live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>angry</td>\n",
       "      <td>u r ridiculous</td>\n",
       "      <td>i might be ridiculous but i am telling the tru...</td>\n",
       "      <td>u little disgusting whore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>others</td>\n",
       "      <td>just for time pass</td>\n",
       "      <td>wt do u do a a living then</td>\n",
       "      <td>maybe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id   label                turn1  \\\n",
       "0  0  others  dont worry ism girl   \n",
       "1  1   angry          when did in   \n",
       "2  2  others                   by   \n",
       "3  3   angry       u r ridiculous   \n",
       "4  4  others   just for time pass   \n",
       "\n",
       "                                               turn2  \\\n",
       "0                       hmm how do i know if you are   \n",
       "1                            saw many times i think    \n",
       "2                                   by google chrome   \n",
       "3  i might be ridiculous but i am telling the tru...   \n",
       "4                         wt do u do a a living then   \n",
       "\n",
       "                       turn3  \n",
       "0            whats ur name ?  \n",
       "1       no . i never saw you  \n",
       "2             where you live  \n",
       "3  u little disgusting whore  \n",
       "4                      maybe  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import *\n",
    "df = functions.parse_file(r\"raw_data/EmoContext/train3.txt\", \"EmoContext\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "for idx,row in df.iterrows():\n",
    "    text_data.append(\"{}. {}. {}.\".format(row['turn1'], row['turn2'], row['turn3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_WORDS = 8000\n",
    "MAX_PROP_LENGTH = 128\n",
    "tokenizer = Tokenizer(num_words=NR_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,split=' ')\n",
    "\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(text_data)\n",
    "X_train = pad_sequences(X_train, maxlen = MAX_PROP_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector(word,label=None):\n",
    "    words = {\"others\": 0, \"angry\": 1, \"sad\":2, \"happy\": 3}\n",
    "    if label == None:\n",
    "        y = [0,0,0,0]\n",
    "        y[words[word]] = 1\n",
    "        return y\n",
    "    if label == word:\n",
    "        return [1,0]\n",
    "    return [0,1]\n",
    "\n",
    "Y_train = []\n",
    "\n",
    "for idx,row in df.iterrows():\n",
    "    Y_train.append(one_hot_vector(row['label']))\n",
    "\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>turn1</th>\n",
       "      <th>turn2</th>\n",
       "      <th>turn3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>angry</td>\n",
       "      <td>then dont ask me</td>\n",
       "      <td>youre a guy not as if you would understand</td>\n",
       "      <td>im not a guy fuck off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>others</td>\n",
       "      <td>mixed things such as</td>\n",
       "      <td>the things you do .</td>\n",
       "      <td>have you seen minions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>happy</td>\n",
       "      <td>today ism very happy</td>\n",
       "      <td>and ism happy for you</td>\n",
       "      <td>i will be marry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>others</td>\n",
       "      <td>noah bring me some</td>\n",
       "      <td>left it there oops</td>\n",
       "      <td>orb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>others</td>\n",
       "      <td>it is thoo</td>\n",
       "      <td>i said soon master .</td>\n",
       "      <td>he is pressuring me</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id   label                  turn1  \\\n",
       "0  0   angry       then dont ask me   \n",
       "1  1  others  mixed things such as    \n",
       "2  2   happy   today ism very happy   \n",
       "3  3  others     noah bring me some   \n",
       "4  4  others             it is thoo   \n",
       "\n",
       "                                        turn2                   turn3  \n",
       "0  youre a guy not as if you would understand   im not a guy fuck off  \n",
       "1                         the things you do .  have you seen minions   \n",
       "2                      and ism happy for you          i will be marry  \n",
       "3                          left it there oops                     orb  \n",
       "4                        i said soon master .     he is pressuring me  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = functions.parse_file(r\"raw_data/EmoContext/devwithlabels3.txt\", \"EmoContext\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "for idx,row in df.iterrows():\n",
    "    text_data.append(\"{}. {}. {}.\".format(row['turn1'], row['turn2'], row['turn3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = Tokenizer(num_words=NR_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "#                                   lower=True,split=' ')\n",
    "\n",
    "#tokenizer.fit_on_texts(text_data)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(text_data)\n",
    "X_test = pad_sequences(X_test, maxlen = MAX_PROP_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector(word,label=None):\n",
    "    words = {\"others\": 0, \"angry\": 1, \"sad\":2, \"happy\": 3}\n",
    "    if label == None:\n",
    "        y = [0,0,0,0]\n",
    "        y[words[word]] = 1\n",
    "        return y\n",
    "    if label == word:\n",
    "        return [1,0]\n",
    "    return [0,1]\n",
    "\n",
    "Y_test = []\n",
    "\n",
    "for idx,row in df.iterrows():\n",
    "    Y_test.append(one_hot_vector(row['label']))\n",
    "\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 32\n",
    "batch_size = 64\n",
    "\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "rmsprop = optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "model = Sequential()\n",
    "model.name = \"ingrid_model\"\n",
    "model.add(Embedding(NR_WORDS, embed_dim,input_length = X_train.shape[1],name=\"ingrid_embedding_model\",trainable=True))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(LSTM(lstm_out, dropout=0.5))\n",
    "model.add(Dense(4,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer=rmsprop, metrics = ['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "ingrid_embedding_model (Embe (None, 128, 128)          1024000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                20608     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 1,044,740\n",
      "Trainable params: 1,044,740\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdcheck = ModelCheckpoint(\"trained_models/best_model_val_acc{val_acc:.4f}.h5\", monitor='val_f1', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25133 samples, validate on 2755 samples\n",
      "Epoch 1/2\n",
      "25133/25133 [==============================] - 50s 2ms/step - loss: 2.6190 - acc: 0.7439 - f1: 0.7065 - val_loss: 0.6313 - val_acc: 0.7996 - val_f1: 0.5915\n",
      "Epoch 2/2\n",
      "25133/25133 [==============================] - 49s 2ms/step - loss: 1.9526 - acc: 0.8162 - f1: 0.7998 - val_loss: 0.6626 - val_acc: 0.7782 - val_f1: 0.5766\n",
      "Train on 25133 samples, validate on 2755 samples\n",
      "Epoch 1/2\n",
      "25133/25133 [==============================] - 49s 2ms/step - loss: 1.8541 - acc: 0.8286 - f1: 0.8119 - val_loss: 0.7343 - val_acc: 0.7408 - val_f1: 0.5512\n",
      "Epoch 2/2\n",
      "25133/25133 [==============================] - 48s 2ms/step - loss: 1.6870 - acc: 0.8436 - f1: 0.8293 - val_loss: 0.6555 - val_acc: 0.7884 - val_f1: 0.5724\n",
      "Train on 25133 samples, validate on 2755 samples\n",
      "Epoch 1/2\n",
      "25133/25133 [==============================] - 49s 2ms/step - loss: 1.6763 - acc: 0.8449 - f1: 0.8317 - val_loss: 0.6382 - val_acc: 0.7960 - val_f1: 0.5920\n",
      "Epoch 2/2\n",
      "25133/25133 [==============================] - 51s 2ms/step - loss: 1.5836 - acc: 0.8514 - f1: 0.8400 - val_loss: 0.7169 - val_acc: 0.7586 - val_f1: 0.5661\n",
      "Train on 25133 samples, validate on 2755 samples\n",
      "Epoch 1/2\n",
      "25133/25133 [==============================] - 50s 2ms/step - loss: 1.5955 - acc: 0.8523 - f1: 0.8408 - val_loss: 0.7187 - val_acc: 0.7546 - val_f1: 0.5630\n",
      "Epoch 2/2\n",
      "25133/25133 [==============================] - 48s 2ms/step - loss: 1.5421 - acc: 0.8569 - f1: 0.8420 - val_loss: 0.7075 - val_acc: 0.7644 - val_f1: 0.5644\n",
      "Train on 25134 samples, validate on 2755 samples\n",
      "Epoch 1/2\n",
      "25134/25134 [==============================] - 49s 2ms/step - loss: 1.5596 - acc: 0.8564 - f1: 0.8441 - val_loss: 0.7742 - val_acc: 0.7376 - val_f1: 0.5393\n",
      "Epoch 2/2\n",
      "25134/25134 [==============================] - 49s 2ms/step - loss: 1.5218 - acc: 0.8613 - f1: 0.8477 - val_loss: 0.7715 - val_acc: 0.7445 - val_f1: 0.5506\n",
      "Train on 25134 samples, validate on 2755 samples\n",
      "Epoch 1/2\n",
      "25134/25134 [==============================] - 50s 2ms/step - loss: 1.5151 - acc: 0.8632 - f1: 0.8502 - val_loss: 0.8419 - val_acc: 0.7180 - val_f1: 0.5345\n",
      "Epoch 2/2\n",
      "24128/25134 [===========================>..] - ETA: 1s - loss: 1.4950 - acc: 0.8638 - f1: 0.8519"
     ]
    }
   ],
   "source": [
    "Y_train = np.array(Y_train)\n",
    "total = len(Y_train)\n",
    "from sklearn.model_selection import KFold\n",
    "# prepare cross validation\n",
    "kfold = KFold(n_splits=6)\n",
    "Y_train = np.array(Y_train)\n",
    "# enumerate splits\n",
    "for train, validation in kfold.split(X_train):\n",
    "    history = model.fit(X_train[train], Y_train[train],\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    epochs=2, verbose=1, batch_size=batch_size,class_weight={\n",
    "                        0: len(X_train[train]) / len(np.where(Y_train[train][:,0]==1.0)[0]),\n",
    "                        1: len(X_train[train]) / len(np.where(Y_train[train][:,1]==1.0)[0]),\n",
    "                        2: len(X_train[train]) / len(np.where(Y_train[train][:,2]==1.0)[0]),\n",
    "                        3: len(X_train[train]) / len(np.where(Y_train[train][:,3]==1.0)[0]),\n",
    "                    })\n",
    "#history = model.fit(X_train, Y_train,\n",
    "#                    validation_data=(X_test, Y_test),\n",
    "#                    epochs=20, verbose=1, batch_size=batch_size,\n",
    "#                    class_weight={\n",
    "#                        0: total / len(np.where(Y_train[:,0]==1.0)[0]),\n",
    "#                        1: total / len(np.where(Y_train[:,1]==1.0)[0]),\n",
    "#                        2: total / len(np.where(Y_train[:,2]==1.0)[0]),\n",
    "#                        3: total / len(np.where(Y_train[:,3]==1.0)[0]),\n",
    "#                    },callbacks=[mdcheck], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model,indices=[{\"indices\":[\"turn1\",\"turn2\",\"turn3\"],\"NR_WORDS\":NR_WORDS,\"MAX_PROP_LENGTH\":MAX_PROP_LENGTH}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = functions.parse_file(r\"raw_data/EmoContext/devwithlabels3.txt\", \"EmoContext\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "\n",
    "for idx,row in df_test.iterrows():\n",
    "    text_data.append(\"{}. {}. {}.\".format(row['turn1'], row['turn2'], row['turn3']))\n",
    "\n",
    "res = model.predict(X_test, batch_size=64, verbose=1)\n",
    "res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revers_words = {0:\"others\", 1:\"angry\", 2:\"sad\", 3:\"happy\"}\n",
    "\n",
    "def softmax_convert(res):\n",
    "    max_i = 0\n",
    "    max_v = 0\n",
    "    for i in range(0,4):\n",
    "        if res[i] > max_v:\n",
    "            max_v = res[i]\n",
    "            max_i = i\n",
    "    return revers_words[max_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for r in res:\n",
    "    results.append(softmax_convert(r))\n",
    "    \n",
    "df_test['label'] = results\n",
    "df_test.head(50)\n",
    "df_test.to_csv(\"ingrid_model.txt\",index=False , sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
