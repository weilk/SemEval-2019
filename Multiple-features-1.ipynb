{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, LSTM, concatenate, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>turn1</th>\n",
       "      <th>turn2</th>\n",
       "      <th>turn3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Don't worry  I'm girl</td>\n",
       "      <td>hmm how do I know if you are</td>\n",
       "      <td>What's ur name?</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>When did I?</td>\n",
       "      <td>saw many times i think -_-</td>\n",
       "      <td>No. I never saw you</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>By</td>\n",
       "      <td>by Google Chrome</td>\n",
       "      <td>Where you live</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>U r ridiculous</td>\n",
       "      <td>I might be ridiculous but I am telling the truth.</td>\n",
       "      <td>U little disgusting whore</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Just for time pass</td>\n",
       "      <td>wt do u do 4 a living then</td>\n",
       "      <td>Maybe</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>I'm a dog person</td>\n",
       "      <td>youre so rude</td>\n",
       "      <td>Whaaaat why</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>So whatsup</td>\n",
       "      <td>Nothing much. Sitting sipping and watching TV....</td>\n",
       "      <td>What are you watching on tv?</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Ok</td>\n",
       "      <td>ok im back!!</td>\n",
       "      <td>So, how are u</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Really?</td>\n",
       "      <td>really really really really really</td>\n",
       "      <td>Y saying so many times...i can hear you</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Bay</td>\n",
       "      <td>in the bay</td>\n",
       "      <td>üòò love you</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>I hate my boyfriend</td>\n",
       "      <td>you got a boyfriend?</td>\n",
       "      <td>Yes</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>I will do night.</td>\n",
       "      <td>Alright. Keep me in loop.</td>\n",
       "      <td>Not giving WhatsApp no.</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Sure go ahead</td>\n",
       "      <td>Many thanks once again!</td>\n",
       "      <td>Love you too</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Bad bad! That's the bad kind of bad.</td>\n",
       "      <td>I have no gf</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Ok get it......</td>\n",
       "      <td>I made it an option</td>\n",
       "      <td>Ok</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Money money and lots of moneyüòçüòç</td>\n",
       "      <td>I need to get it tailored but I'm in love with...</td>\n",
       "      <td>üòÅüòÅ</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>My gf left ne</td>\n",
       "      <td>Get over it. Go out with someone else.</td>\n",
       "      <td>Me*</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>get lost</td>\n",
       "      <td>I know you guys want to loose to me always.</td>\n",
       "      <td>I don't want to talk u any more</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>You are lying and i know that</td>\n",
       "      <td>I KNOW YOU'RE LYING, AB BYS</td>\n",
       "      <td>üò≠üò≠</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>Ur creator is very bad</td>\n",
       "      <td>you are only the creator of your brain.</td>\n",
       "      <td>üòë</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                            turn1  \\\n",
       "0    0            Don't worry  I'm girl   \n",
       "1    1                      When did I?   \n",
       "2    2                               By   \n",
       "3    3                   U r ridiculous   \n",
       "4    4               Just for time pass   \n",
       "5    5                 I'm a dog person   \n",
       "6    6                       So whatsup   \n",
       "7    7                               Ok   \n",
       "8    8                          Really?   \n",
       "9    9                              Bay   \n",
       "10  10              I hate my boyfriend   \n",
       "11  11                 I will do night.   \n",
       "12  12                    Sure go ahead   \n",
       "13  13                              Bad   \n",
       "14  14                  Ok get it......   \n",
       "15  15  Money money and lots of moneyüòçüòç   \n",
       "16  16                    My gf left ne   \n",
       "17  17                         get lost   \n",
       "18  18    You are lying and i know that   \n",
       "19  19           Ur creator is very bad   \n",
       "\n",
       "                                                turn2  \\\n",
       "0                        hmm how do I know if you are   \n",
       "1                          saw many times i think -_-   \n",
       "2                                    by Google Chrome   \n",
       "3   I might be ridiculous but I am telling the truth.   \n",
       "4                          wt do u do 4 a living then   \n",
       "5                                       youre so rude   \n",
       "6   Nothing much. Sitting sipping and watching TV....   \n",
       "7                                        ok im back!!   \n",
       "8                  really really really really really   \n",
       "9                                          in the bay   \n",
       "10                               you got a boyfriend?   \n",
       "11                          Alright. Keep me in loop.   \n",
       "12                            Many thanks once again!   \n",
       "13               Bad bad! That's the bad kind of bad.   \n",
       "14                                I made it an option   \n",
       "15  I need to get it tailored but I'm in love with...   \n",
       "16             Get over it. Go out with someone else.   \n",
       "17        I know you guys want to loose to me always.   \n",
       "18                        I KNOW YOU'RE LYING, AB BYS   \n",
       "19            you are only the creator of your brain.   \n",
       "\n",
       "                                      turn3   label  \n",
       "0                           What's ur name?  others  \n",
       "1                       No. I never saw you   angry  \n",
       "2                            Where you live  others  \n",
       "3                 U little disgusting whore   angry  \n",
       "4                                     Maybe  others  \n",
       "5                               Whaaaat why  others  \n",
       "6              What are you watching on tv?  others  \n",
       "7                             So, how are u  others  \n",
       "8   Y saying so many times...i can hear you  others  \n",
       "9                                üòò love you  others  \n",
       "10                                      Yes   angry  \n",
       "11                  Not giving WhatsApp no.  others  \n",
       "12                             Love you too  others  \n",
       "13                             I have no gf     sad  \n",
       "14                                       Ok  others  \n",
       "15                                       üòÅüòÅ   happy  \n",
       "16                                      Me*     sad  \n",
       "17          I don't want to talk u any more   angry  \n",
       "18                                       üò≠üò≠     sad  \n",
       "19                                        üòë     sad  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import *\n",
    "df = functions.parse_file(r\"raw_data/EmoContext/train.txt\", \"EmoContext\")\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = functions.parse_file(r\"testwithoutlabels.txt\", \"EmoContext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "for idx,row in df.iterrows():\n",
    "    text_data.append(\" {} fullstop {} fullstop {} fullstop\".format(row['turn1'], row['turn2'], row['turn3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_data = []\n",
    "for idx,row in df_test.iterrows():\n",
    "    test_text_data.append(\" {} fullstop {} fullstop {} fullstop\".format(row['turn1'], row['turn2'], row['turn3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_map = {\n",
    "    'üòò': ' emoticon',\n",
    "    'üòç': ' happyemoticon',\n",
    "    'üòÅ': ' happyemoticon',\n",
    "    'üò≠': ' sademoticon',\n",
    "    'üòë': ' sademoticon',\n",
    "    'üòª': ' happyemoticon',\n",
    "    'üòÇ': ' happyemoticon',\n",
    "    'üëç': ' emoticon',\n",
    "    'üòÄ': ' happyemoticon',\n",
    "    ':D': ' happyemoticon',\n",
    "    'üôÇ':  ' happyemoticon',\n",
    "    '<3': ' happyemoticon',\n",
    "    'üòì' : ' sademoticon',\n",
    "    'üòí' : ' angryemoticon',\n",
    "    'üòà' : ' emoticon',\n",
    "    'üëø' : ' angryemoticon',\n",
    "    'üñë' : ' happyemoticon',\n",
    "    'üòæ' : ' emoticon',\n",
    "    'üò†' : ' angryemoticon',\n",
    "    'üëª' : ' emoticon',\n",
    "    ':(' : ' sademoticon',\n",
    "    ':)' : ' happyemoticon',\n",
    "    'xD' : ' happyemoticon',\n",
    "    'üíî' : ' sademoticon',\n",
    "    'üò•' : ' emoticon',\n",
    "    'üòû' : ' sademoticon',\n",
    "    'üò§' : ' angryemoticon',\n",
    "    'üòÉ' : ' happyemoticon',\n",
    "    'üò¶' : ' sademoticon',\n",
    "    ':3' : ' emoticon',\n",
    "    'üòº' : ' emoticon',\n",
    "    'üòè' : ' happyemoticon',\n",
    "    'üò±' : ' sademoticon',\n",
    "    'üò¨' : ' sademoticon',\n",
    "    'üôÅ' : ' sademoticon',\n",
    "    '</3' : ' sademoticon',\n",
    "    'üò∫' : ' happyemoticon',\n",
    "    'üò£' : ' angryemoticon',\n",
    "    'üò¢' : ' sademoticon',\n",
    "    'üòÜ' : ' happyemoticon',\n",
    "    'üòÑ' : ' happyemoticon',\n",
    "    'üòÖ' : ' happyemoticon',\n",
    "    ':-)' : ' happyemoticon',\n",
    "    'üòä' : ' happyemoticon',\n",
    "    'üòï' : ' sademoticon',\n",
    "    'üòΩ' : ' happyemoticon',\n",
    "    'üôÄ' : ' angryemoticon',\n",
    "    'ü§£' : ' happyemoticon',\n",
    "    'ü§ê' : ' emoticon',\n",
    "    'üò°' : ' sademoticon',\n",
    "    'üëå' : ' happyemoticon', \n",
    "    'üòÆ' : ' emoticon',\n",
    "    '‚ù§Ô∏è' : ' happyemoticon',\n",
    "    'üôÑ' : ' happyemoticon',\n",
    "    'üòø' : ' sademoticon',\n",
    "    'üòâ' : ' happyemoticon',\n",
    "    'üòã' : ' happyemoticon',\n",
    "    'üòê' : ' emoticon',\n",
    "    'üòπ' : ' happyemoticon',\n",
    "    'üò¥' : ' sademoticon',\n",
    "    'üí§' : ' emoticon',\n",
    "    'üòú' : ' happyemoticon',\n",
    "    'üòá' : ' happyemoticon',\n",
    "    'üòî' : ' sademoticon',\n",
    "    'üò©' : ' sademoticon',\n",
    "    '‚ù§' : ' happyemoticon',\n",
    "    'üò≤' : ' emoticon',\n",
    "    'üò´' : ' sademoticon',\n",
    "    'üò≥' : ' sademoticon',\n",
    "    'üò∞' : ' sademoticon',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words_df = pd.read_csv('bad-words.csv')\n",
    "#bad_words_df['jigaboo']\n",
    "bad_words = list(bad_words_df['jigaboo'])\n",
    "bad_words.sort(key = lambda s: len(s))\n",
    "bad_words = bad_words[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(text_data)):\n",
    "    for k in emoticons_map.keys():\n",
    "        if text_data[i].find(k) >= 0:\n",
    "            text_data[i] = text_data[i].replace(k, emoticons_map[k])\n",
    "    for bw in bad_words:\n",
    "        bw = \" {} \".format(bw)\n",
    "        if text_data[i].find(bw) >= 0:\n",
    "            text_data[i] = text_data[i].replace(bw, ' badword ')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(test_text_data)):\n",
    "    for k in emoticons_map.keys():\n",
    "        if test_text_data[i].find(k) >= 0:\n",
    "            test_text_data[i] = test_text_data[i].replace(k, emoticons_map[k])\n",
    "    for bw in bad_words:\n",
    "        bw = \" {} \".format(bw)\n",
    "        if test_text_data[i].find(bw) >= 0:\n",
    "            test_text_data[i] = test_text_data[i].replace(bw, ' badword ')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from autocorrect import spell\n",
    "#from spellchecker import SpellChecker\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lem = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "reserved_words = ['badword','fullstop','happyemoticon','sademoticon','angryemoticon']\n",
    "\n",
    "#spell = SpellChecker()\n",
    "\n",
    "def get_word_senti_score(word):\n",
    "    good = list(swn.senti_synsets(word))\n",
    "    posscore=0\n",
    "    negscore=0\n",
    "    for synst in good:\n",
    "\n",
    "        posscore=posscore+synst.pos_score()\n",
    "        negscore=negscore+synst.neg_score()\n",
    "\n",
    "    posscore = posscore/len(good) if len(good)>0 else 0\n",
    "    negscore = negscore/len(good) if len(good)>0 else 0\n",
    "    \n",
    "    return posscore, negscore\n",
    "\n",
    "\n",
    "def filter_text(text):    \n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    #words = [w for w in words if not w in stop_words]\n",
    "    stemmed = [lem.lemmatize(word) if not word in reserved_words else word for word in tokens]\n",
    "    corrected = [spell(word) if not word in reserved_words else word for word in stemmed ]\n",
    "    corrected = [word.lower() for word in corrected]\n",
    "    #corrected = [spell.correction(word) for word in stemmed]\n",
    "    tagged = pos_tag(corrected)\n",
    "    tagged = np.array(tagged)\n",
    "    \n",
    "    scores = []\n",
    "    for word in corrected:\n",
    "        posscore, negscore = get_word_senti_score(word)\n",
    "        scores.append([posscore, negscore])\n",
    "    \n",
    "    return tagged, np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_data = []\n",
    "tokenized_test_data = []\n",
    "tagged_train_data = []\n",
    "tagged_test_data = []\n",
    "pos_scores_train_data = []\n",
    "neg_scores_train_data = []\n",
    "pos_scores_test_data = []\n",
    "neg_scores_test_data = []\n",
    "\n",
    "\n",
    "\n",
    "i=0\n",
    "for sentence in text_data:\n",
    "    taggs, scores = filter_text(sentence)\n",
    "    tokenized_train_data.append(taggs[:,0])\n",
    "    tagged_train_data.append(taggs[:,1])\n",
    "    pos_scores_train_data.append(scores[:,0])\n",
    "    neg_scores_train_data.append(scores[:,1])\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for sentence in test_text_data:\n",
    "    taggs, scores = filter_text(sentence)\n",
    "    tokenized_test_data.append(taggs[:,0])\n",
    "    tagged_test_data.append(taggs[:,1])\n",
    "    pos_scores_test_data.append(scores[:,0])\n",
    "    neg_scores_test_data.append(scores[:,1])\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    i+=1\n",
    "\n",
    "unique_tokenized_words = {}\n",
    "\n",
    "for words in tokenized_train_data:\n",
    "    for word in words:\n",
    "        if word not in unique_tokenized_words:\n",
    "            unique_tokenized_words[word] = 1\n",
    "        else:\n",
    "            unique_tokenized_words[word] += 1\n",
    "            \n",
    "for words in tokenized_test_data:\n",
    "    for word in words:\n",
    "        if word not in unique_tokenized_words:\n",
    "            unique_tokenized_words[word] = 1\n",
    "        else:\n",
    "            unique_tokenized_words[word] += 1\n",
    "\n",
    "sorted_unique_tokenized_words = sorted(unique_tokenized_words.items(), key=lambda kv: kv[1], reverse=True)\n",
    "token_values = {}\n",
    "i = 1\n",
    "for word in sorted_unique_tokenized_words:\n",
    "    token_values[word[0]] = i\n",
    "    i += 1\n",
    "\n",
    "final_tokenized_train_data = []\n",
    "for i in range(0,len(tokenized_train_data)):\n",
    "    sentence = []\n",
    "    for word in tokenized_train_data[i]:\n",
    "        sentence.append(token_values[word])\n",
    "    final_tokenized_train_data.append(sentence)\n",
    "\n",
    "final_tokenized_test_data = []\n",
    "for i in range(0,len(tokenized_test_data)):\n",
    "    sentence = []\n",
    "    for word in tokenized_test_data[i]:\n",
    "        sentence.append(token_values[word])\n",
    "    final_tokenized_test_data.append(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unique_tagged_words = {}\n",
    "\n",
    "for words in tagged_train_data:\n",
    "    for word in words:\n",
    "        if word not in unique_tagged_words:\n",
    "            unique_tagged_words[word] = 1\n",
    "        else:\n",
    "            unique_tagged_words[word] += 1\n",
    "            \n",
    "for words in tagged_test_data:\n",
    "    for word in words:\n",
    "        if word not in unique_tagged_words:\n",
    "            unique_tagged_words[word] = 1\n",
    "        else:\n",
    "            unique_tagged_words[word] += 1\n",
    "\n",
    "sorted_unique_tagged_words = sorted(unique_tagged_words.items(), key=lambda kv: kv[1], reverse=True)\n",
    "tagged_values = {}\n",
    "i = 1\n",
    "for word in sorted_unique_tagged_words:\n",
    "    tagged_values[word[0]] = i\n",
    "    i += 1\n",
    "\n",
    "final_tagged_train_data = []\n",
    "for i in range(0,len(tagged_train_data)):\n",
    "    sentence = []\n",
    "    for word in tagged_train_data[i]:\n",
    "        sentence.append(tagged_values[word])\n",
    "    final_tagged_train_data.append(sentence)\n",
    "\n",
    "final_tagged_test_data = []\n",
    "for i in range(0,len(tagged_test_data)):\n",
    "    sentence = []\n",
    "    for word in tagged_test_data[i]:\n",
    "        sentence.append(tagged_values[word])\n",
    "    final_tagged_test_data.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13189"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_WORDS1 = len(unique_tokenized_words)+1\n",
    "NR_WORDS2 = len(unique_tagged_words)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"final_tokenized_train_data.npy\", final_tokenized_train_data)\n",
    "np.save(\"final_tokenized_test_data.npy\", final_tokenized_test_data)\n",
    "np.save(\"final_tagged_train_data.npy\", final_tagged_train_data)\n",
    "np.save(\"final_tagged_test_data.npy\", final_tagged_test_data)\n",
    "np.save(\"pos_scores_train_data.npy\", pos_scores_train_data)\n",
    "np.save(\"pos_scores_test_data.npy\", pos_scores_test_data)\n",
    "np.save(\"neg_scores_train_data.npy\", neg_scores_train_data)\n",
    "np.save(\"neg_scores_test_data.npy\", neg_scores_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tokenized_train_data = np.load(\"final_tokenized_train_data.npy\")\n",
    "final_tokenized_test_data = np.load(\"final_tokenized_test_data.npy\")\n",
    "final_tagged_train_data = np.load(\"final_tagged_train_data.npy\")\n",
    "final_tagged_test_data = np.load(\"final_tagged_test_data.npy\")\n",
    "pos_scores_train_data = np.load(\"pos_scores_train_data.npy\")\n",
    "pos_scores_test_data = np.load(\"pos_scores_test_data.npy\")\n",
    "neg_scores_train_data = np.load(\"neg_scores_train_data.npy\")\n",
    "neg_scores_test_data = np.load(\"neg_scores_test_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = pad_sequences(final_tokenized_train_data)\n",
    "X2 = pad_sequences(final_tagged_train_data)\n",
    "X3 = pad_sequences(pos_scores_train_data)\n",
    "X4 = pad_sequences(neg_scores_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {\"others\": 0, \"angry\": 1, \"sad\":2, \"happy\": 3}\n",
    "\n",
    "def one_hot_vector(word):\n",
    "    y = [0,0,0,0]\n",
    "    y[words[word]] = 1\n",
    "    return y\n",
    "\n",
    "Y = []\n",
    "\n",
    "for idx,row in df.iterrows():\n",
    "    Y.append(one_hot_vector(row['label']))\n",
    "\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13190"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NR_WORDS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NR_WORDS2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 64\n",
    "batch_size = 128\n",
    "\n",
    "model1_input = Input(shape=(X1.shape[1],))\n",
    "model1 = Embedding(NR_WORDS1, 256, input_length = X1.shape[1])(model1_input)\n",
    "model1 = LSTM(lstm_out)(model1)\n",
    "model2_input = Input(shape=(X2.shape[1],))\n",
    "model2 = Embedding(NR_WORDS2, 64, input_length = X2.shape[1])(model2_input)\n",
    "model2 = LSTM(lstm_out)(model2)\n",
    "\n",
    "model3_input = Input(shape=(X3.shape[1],))\n",
    "model3 = Dense(64, activation='sigmoid')(model3_input)\n",
    "model4_input = Input(shape=(X4.shape[1],))\n",
    "model4 = Dense(64, activation='sigmoid')(model4_input)\n",
    "\n",
    "merged = concatenate([model1, model2])\n",
    "merged = Dense(64,activation='relu')(merged)\n",
    "merged = Dropout(0.3)(merged)\n",
    "merged = Dense(4,activation='softmax')(merged)\n",
    "\n",
    "model = Model(inputs=[model1_input, model2_input], outputs=merged)\n",
    "\n",
    "#model = LSTM(lstm_out)(model)\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(4,activation='softmax'))\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "rmsprop = optimizers.RMSprop(lr=0.005, rho=0.9, epsilon=None, decay=0.0)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer=rmsprop, metrics = ['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 164)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 164)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 164, 256)     3376640     input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 164, 64)      2496        input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 64)           82176       embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (None, 64)           33024       embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128)          0           lstm_9[0][0]                     \n",
      "                                                                 lstm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 64)           8256        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64)           0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 4)            260         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,502,852\n",
      "Trainable params: 3,502,852\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_1, X_test_1, Y_train, Y_test = train_test_split(X1,Y, test_size = 0, random_state = 42)\n",
    "#X_train_2, X_test_2, Y_train_2, Y_test_2 = train_test_split(X2,Y, test_size = 0, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit([X1, X2, X3, X4], Y, batch_size = 128, nb_epoch = 3, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K.set_value(model.optimizer.lr, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22620 samples, validate on 7540 samples\n",
      "Epoch 1/1\n",
      "22620/22620 [==============================] - 161s 7ms/step - loss: 0.2771 - acc: 0.8916 - f1: 0.7221 - val_loss: 0.2189 - val_acc: 0.9089 - val_f1: 0.7979\n",
      "Train on 22620 samples, validate on 7540 samples\n",
      "Epoch 1/1\n",
      "22620/22620 [==============================] - 161s 7ms/step - loss: 0.1812 - acc: 0.9304 - f1: 0.8466 - val_loss: 0.1639 - val_acc: 0.9360 - val_f1: 0.8573\n",
      "Train on 22620 samples, validate on 7540 samples\n",
      "Epoch 1/1\n",
      "22620/22620 [==============================] - 161s 7ms/step - loss: 0.1484 - acc: 0.9446 - f1: 0.8776 - val_loss: 0.1450 - val_acc: 0.9456 - val_f1: 0.8806\n",
      "Train on 22620 samples, validate on 7540 samples\n",
      "Epoch 1/1\n",
      "22620/22620 [==============================] - 163s 7ms/step - loss: 0.1250 - acc: 0.9538 - f1: 0.8966 - val_loss: 0.1174 - val_acc: 0.9560 - val_f1: 0.9024\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# prepare cross validation\n",
    "kfold = KFold(n_splits=4)\n",
    "Y = np.array(Y)\n",
    "# enumerate splits\n",
    "for train, validation in kfold.split(X1):\n",
    "    history = model.fit([X1[train], X2[train]], Y[train],\n",
    "                    validation_data=([X1[validation], X2[validation]], Y[validation]),\n",
    "                    epochs=1, verbose=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-6c813e833929>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mev_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Score: %.3f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mev_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation Accuracy: %.3f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mev_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"F1 score: %.3f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mev_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "ev_result = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"Score: %.3f\" % (ev_result[0]))\n",
    "print(\"Validation Accuracy: %.3f\" % (ev_result[1]))\n",
    "print(\"F1 score: %.3f\" % (ev_result[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"multiple_features.json\", \"w\") as outfile:\n",
    "    outfile.write(model_json)\n",
    "model.save_weights(\"multiple_features.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_test = pad_sequences(final_tokenized_test_data, maxlen=X1.shape[1])\n",
    "X2_test = pad_sequences(final_tagged_test_data, maxlen=X2.shape[1])\n",
    "X3_test = pad_sequences(pos_scores_test_data, maxlen=X3.shape[1])\n",
    "X4_test = pad_sequences(neg_scores_test_data, maxlen=X4.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5509/5509 [==============================] - 10s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "res = model.predict([X1_test, X2_test], batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "revers_words = {0:\"others\", 1:\"angry\", 2:\"sad\", 3:\"happy\"}\n",
    "\n",
    "def softmax_convert(res):\n",
    "    max_i = 0\n",
    "    max_v = 0\n",
    "    for i in range(0,4):\n",
    "        if res[i] > max_v:\n",
    "            max_v = res[i]\n",
    "            max_i = i\n",
    "    return revers_words[max_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for r in res:\n",
    "    results.append(softmax_convert(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['label'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>turn1</th>\n",
       "      <th>turn2</th>\n",
       "      <th>turn3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hmm</td>\n",
       "      <td>What does your bio mean?</td>\n",
       "      <td>I don‚Äôt have any bio</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What you like</td>\n",
       "      <td>very little things</td>\n",
       "      <td>Ok</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>How so?</td>\n",
       "      <td>I want to fuck babu</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>what did you guess</td>\n",
       "      <td>what what</td>\n",
       "      <td>fuck</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>We ?</td>\n",
       "      <td>of course we will!</td>\n",
       "      <td>What gender movies you like??</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Where are you now?</td>\n",
       "      <td>At home just about to have breakfast...</td>\n",
       "      <td>what are you eating?</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>That was a joke btw...</td>\n",
       "      <td>it was</td>\n",
       "      <td>Yes üòÅ</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Who d hell s he</td>\n",
       "      <td>johnny depp...duh?</td>\n",
       "      <td>Who she</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>yes, good advice</td>\n",
       "      <td>best advice ...</td>\n",
       "      <td>i great thx</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Nice to meet u</td>\n",
       "      <td>Hi, nice to meet you too! üò∏üòÇ</td>\n",
       "      <td>üòÅ</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Yupp</td>\n",
       "      <td>why?</td>\n",
       "      <td>Don't know I'm tired</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Software</td>\n",
       "      <td>Software what? I plan on going for development...</td>\n",
       "      <td>I am into android development stuff</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Very nice</td>\n",
       "      <td>Thanks!! :)</td>\n",
       "      <td>R u know tamil</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>First  you  hurt me</td>\n",
       "      <td>okay</td>\n",
       "      <td>So I talked  rude</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Love you üôä</td>\n",
       "      <td>you don't recognize me? üòè</td>\n",
       "      <td>I love you üôä</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>In India Fogg is going on</td>\n",
       "      <td>MumbaiIndians getting routed!!!</td>\n",
       "      <td>Ohh really</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>is my grammar perfect or should I need to lear...</td>\n",
       "      <td>Yes, it is possible.</td>\n",
       "      <td>thank you</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>by</td>\n",
       "      <td>In Suits.</td>\n",
       "      <td>have good day</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>I don‚Äôt know what to write</td>\n",
       "      <td>what are you writing about?</td>\n",
       "      <td>for my profile picture I mean</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>so you make jokes. i already got that.</td>\n",
       "      <td>that was a good joke i laughed üëç</td>\n",
       "      <td>do you read newspapers</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>Yeah mee too</td>\n",
       "      <td>Me too! All my friends are also excited</td>\n",
       "      <td>Ohhh so funny</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>Thanks</td>\n",
       "      <td>you're welcome! üòÅüòÅüòÅüòÅ</td>\n",
       "      <td>Can you help me</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Offer?</td>\n",
       "      <td>You've already enrolled in the offer. If this ...</td>\n",
       "      <td>When it start again?</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>Uhhhmm whyy??</td>\n",
       "      <td>not you ...</td>\n",
       "      <td>No im really a bad bitch</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>sure will u cal me ni8</td>\n",
       "      <td>Ok</td>\n",
       "      <td>then</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>About what I just told</td>\n",
       "      <td>I SHOULD</td>\n",
       "      <td>You drunk? üòÇ</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>Yeaaa black panther</td>\n",
       "      <td>Black Panther teaser Don't why it was that hard.</td>\n",
       "      <td>It was an awesome movie</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>now dont give me a chance to be an danger evil</td>\n",
       "      <td>That's because you don't give yourself a chance.</td>\n",
       "      <td>i give</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>Watch spitsvilla at 7 p.m today</td>\n",
       "      <td>both are tough matches, would be fun to watch!</td>\n",
       "      <td>I love that show</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>Uuu</td>\n",
       "      <td>no i'n not !! what's wrong with you ?!?!</td>\n",
       "      <td>I hate uuu</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              turn1  \\\n",
       "0    0                                                Hmm   \n",
       "1    1                                      What you like   \n",
       "2    2                                                Yes   \n",
       "3    3                                 what did you guess   \n",
       "4    4                                               We ?   \n",
       "5    5                                 Where are you now?   \n",
       "6    6                             That was a joke btw...   \n",
       "7    7                                    Who d hell s he   \n",
       "8    8                                   yes, good advice   \n",
       "9    9                                     Nice to meet u   \n",
       "10  10                                               Yupp   \n",
       "11  11                                           Software   \n",
       "12  12                                         Very nice    \n",
       "13  13                                First  you  hurt me   \n",
       "14  14                                         Love you üôä   \n",
       "15  15                          In India Fogg is going on   \n",
       "16  16  is my grammar perfect or should I need to lear...   \n",
       "17  17                                                 by   \n",
       "18  18                         I don‚Äôt know what to write   \n",
       "19  19             so you make jokes. i already got that.   \n",
       "20  20                                       Yeah mee too   \n",
       "21  21                                             Thanks   \n",
       "22  22                                             Offer?   \n",
       "23  23                                      Uhhhmm whyy??   \n",
       "24  24                             sure will u cal me ni8   \n",
       "25  25                             About what I just told   \n",
       "26  26                                Yeaaa black panther   \n",
       "27  27     now dont give me a chance to be an danger evil   \n",
       "28  28                    Watch spitsvilla at 7 p.m today   \n",
       "29  29                                                Uuu   \n",
       "\n",
       "                                                turn2  \\\n",
       "0                            What does your bio mean?   \n",
       "1                                  very little things   \n",
       "2                                             How so?   \n",
       "3                                           what what   \n",
       "4                                  of course we will!   \n",
       "5             At home just about to have breakfast...   \n",
       "6                                              it was   \n",
       "7                                  johnny depp...duh?   \n",
       "8                                     best advice ...   \n",
       "9                        Hi, nice to meet you too! üò∏üòÇ   \n",
       "10                                               why?   \n",
       "11  Software what? I plan on going for development...   \n",
       "12                                        Thanks!! :)   \n",
       "13                                               okay   \n",
       "14                          you don't recognize me? üòè   \n",
       "15                    MumbaiIndians getting routed!!!   \n",
       "16                               Yes, it is possible.   \n",
       "17                                          In Suits.   \n",
       "18                        what are you writing about?   \n",
       "19                   that was a good joke i laughed üëç   \n",
       "20            Me too! All my friends are also excited   \n",
       "21                               you're welcome! üòÅüòÅüòÅüòÅ   \n",
       "22  You've already enrolled in the offer. If this ...   \n",
       "23                                        not you ...   \n",
       "24                                                 Ok   \n",
       "25                                           I SHOULD   \n",
       "26   Black Panther teaser Don't why it was that hard.   \n",
       "27   That's because you don't give yourself a chance.   \n",
       "28     both are tough matches, would be fun to watch!   \n",
       "29           no i'n not !! what's wrong with you ?!?!   \n",
       "\n",
       "                                  turn3   label  \n",
       "0                  I don‚Äôt have any bio  others  \n",
       "1                                   Ok   others  \n",
       "2                   I want to fuck babu   angry  \n",
       "3                                  fuck  others  \n",
       "4         What gender movies you like??  others  \n",
       "5                  what are you eating?  others  \n",
       "6                                 Yes üòÅ   happy  \n",
       "7                               Who she   angry  \n",
       "8                           i great thx  others  \n",
       "9                                     üòÅ   happy  \n",
       "10                 Don't know I'm tired  others  \n",
       "11  I am into android development stuff  others  \n",
       "12                      R u know tamil   others  \n",
       "13                    So I talked  rude   angry  \n",
       "14                         I love you üôä  others  \n",
       "15                           Ohh really  others  \n",
       "16                            thank you  others  \n",
       "17                        have good day  others  \n",
       "18        for my profile picture I mean  others  \n",
       "19               do you read newspapers  others  \n",
       "20                        Ohhh so funny   happy  \n",
       "21                      Can you help me  others  \n",
       "22                 When it start again?  others  \n",
       "23             No im really a bad bitch     sad  \n",
       "24                                 then  others  \n",
       "25                         You drunk? üòÇ   happy  \n",
       "26              It was an awesome movie   happy  \n",
       "27                               i give  others  \n",
       "28                     I love that show  others  \n",
       "29                           I hate uuu   angry  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"test.txt\",index=False , sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_bidirectional = np.load(\"res_bidirectional.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
