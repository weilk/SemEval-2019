{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "df = functions.parse_file(r\"SemEval-2019/raw_data/EmoContext/train3.txt\", \"EmoContext\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "for idx,row in df.iterrows():\n",
    "    text_data.append(\"{}. {}. {}.\".format(row['turn1'], row['turn2'], row['turn3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_WORDS = 8000\n",
    "MAX_PROP_LENGTH = 128\n",
    "tokenizer = Tokenizer(num_words=NR_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,split=' ')\n",
    "\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(text_data)\n",
    "X_train = pad_sequences(X_train, maxlen = MAX_PROP_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector(word,label=None):\n",
    "    words = {\"others\": 0, \"angry\": 1, \"sad\":2, \"happy\": 3}\n",
    "    if label == None:\n",
    "        y = [0,0,0,0]\n",
    "        y[words[word]] = 1\n",
    "        return y\n",
    "    if label == word:\n",
    "        return [1,0]\n",
    "    return [0,1]\n",
    "\n",
    "Y_train = []\n",
    "\n",
    "for idx,row in df.iterrows():\n",
    "    Y_train.append(one_hot_vector(row['label']))\n",
    "\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = functions.parse_file(r\"SemEval-2019/raw_data/EmoContext/test1.txt\", \"EmoContext\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "for idx,row in df.iterrows():\n",
    "    text_data.append(\"{}. {}. {}.\".format(row['turn1'], row['turn2'], row['turn3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = Tokenizer(num_words=NR_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "#                                   lower=True,split=' ')\n",
    "\n",
    "#tokenizer.fit_on_texts(text_data)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(text_data)\n",
    "X_test = pad_sequences(X_test, maxlen = MAX_PROP_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector(word,label=None):\n",
    "    words = {\"others\": 0, \"angry\": 1, \"sad\":2, \"happy\": 3}\n",
    "    if label == None:\n",
    "        y = [0,0,0,0]\n",
    "        y[words[word]] = 1\n",
    "        return y\n",
    "    if label == word:\n",
    "        return [1,0]\n",
    "    return [0,1]\n",
    "\n",
    "Y_test = []\n",
    "\n",
    "for idx,row in df.iterrows():\n",
    "    Y_test.append(one_hot_vector(row['label']))\n",
    "\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 32\n",
    "batch_size = 64\n",
    "\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "rmsprop = optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "model = Sequential()\n",
    "model.name = \"ingrid_model\"\n",
    "model.add(Embedding(NR_WORDS, embed_dim,input_length = X_train.shape[1],name=\"ingrid_embedding_model\",trainable=True))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(LSTM(lstm_out, dropout=0.5))\n",
    "model.add(Dense(4,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer=rmsprop, metrics = ['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdcheck = ModelCheckpoint(\"trained_models/best_model_val_acc{val_acc:.4f}.h5\", monitor='val_f1', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array(Y_train)\n",
    "total = len(Y_train)\n",
    "from sklearn.model_selection import KFold\n",
    "# prepare cross validation\n",
    "kfold = KFold(n_splits=6)\n",
    "Y_train = np.array(Y_train)\n",
    "# enumerate splits\n",
    "for train, validation in kfold.split(X_train):\n",
    "    history = model.fit(X_train[train], Y_train[train],\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    epochs=5, verbose=1, batch_size=batch_size,class_weight={\n",
    "                        0: len(X_train[train]) / len(np.where(Y_train[train][:,0]==1.0)[0]),\n",
    "                        1: len(X_train[train]) / len(np.where(Y_train[train][:,1]==1.0)[0]),\n",
    "                        2: len(X_train[train]) / len(np.where(Y_train[train][:,2]==1.0)[0]),\n",
    "                        3: len(X_train[train]) / len(np.where(Y_train[train][:,3]==1.0)[0]),\n",
    "                    })\n",
    "#history = model.fit(X_train, Y_train,\n",
    "#                    validation_data=(X_test, Y_test),\n",
    "#                    epochs=20, verbose=1, batch_size=batch_size,\n",
    "#                    class_weight={\n",
    "#                        0: total / len(np.where(Y_train[:,0]==1.0)[0]),\n",
    "#                        1: total / len(np.where(Y_train[:,1]==1.0)[0]),\n",
    "#                        2: total / len(np.where(Y_train[:,2]==1.0)[0]),\n",
    "#                        3: total / len(np.where(Y_train[:,3]==1.0)[0]),\n",
    "#                    },callbacks=[mdcheck], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model,indices=[{\"indices\":[\"turn1\",\"turn2\",\"turn3\"],\"NR_WORDS\":NR_WORDS,\"MAX_PROP_LENGTH\":MAX_PROP_LENGTH}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = functions.parse_file(r\"SemEval-2019/raw_data/EmoContext/test1.txt\", \"EmoContext\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "\n",
    "for idx,row in df_test.iterrows():\n",
    "    text_data.append(\"{}. {}. {}.\".format(row['turn1'], row['turn2'], row['turn3']))\n",
    "\n",
    "res = model.predict(X_test, batch_size=64, verbose=1)\n",
    "res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revers_words = {0:\"others\", 1:\"angry\", 2:\"sad\", 3:\"happy\"}\n",
    "\n",
    "def softmax_convert(res):\n",
    "    max_i = 0\n",
    "    max_v = 0\n",
    "    for i in range(0,4):\n",
    "        if res[i] > max_v:\n",
    "            max_v = res[i]\n",
    "            max_i = i\n",
    "    return revers_words[max_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for r in res:\n",
    "    results.append(softmax_convert(r))\n",
    "    \n",
    "df_test['label'] = results\n",
    "df_test.head(50)\n",
    "df_test.to_csv(\"ingrid_model.txt\",index=False , sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
